---
title: "Webscraping 2: Client-side and APIs"
author:
  name: Muhsin Ciftci
  affiliation: Goethe University Frankfurt
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    highlight: tango
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 4
    toc_float: yes
editor_options:
  chunk_output_type: console
---

```{r}
knitr::opts_chunk$set(message = FALSE, warning=FALSE, echo = TRUE, cache = TRUE)
```

```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, httr, lubridate, hrbrthemes, janitor, jsonlite, listviewer, usethis, fredr)

theme_set(theme_light())
rm(list = ls())
```


# Sign-up and software requirements
## Sign-up
We’re going to be downloading economic data from the FRED API. This will require that you first create a user account and then register a personal API key.

## External software
Today I’ll be using `JSONView`, a browser extension that renders JSON output nicely in Chrome and Firefox.

> Recap from last time
During the last lecture, we saw that websites and web applications fall into two categories: 1) Server-side and 2) Client-side. We then practised scraping data that falls into the first category — i.e. rendered server-side — using the `rvest` package. This technique focuses on CSS selectors (with help from SelectorGadget) and HTML tags. We also saw that webscraping often involves as much art as science. The plethora of CSS options and the flexibility of HTML itself means that steps which work perfectly well on one website can easily fail on another website.
Today we focus on the second category: Scraping web data that is rendered client-side. The good news is that, when available, this approach typically makes it much easier to scrape data from the web. The downside is that, again, it can involve as much art as it does science.

## Application 1: Trees of New York City
For this example, we do not use any personal API. 
- Open the website [here](https://data.cityofnewyork.us/Environment/2015-Street-Tree-Census-Tree-Data/uvpi-gqnh)

- Go to website and Click API and copy it.

- Now take it intro R

```{r}
library(jsonlite) ## Already loaded

nyc_trees <- fromJSON("https://data.cityofnewyork.us/resource/nwxe-4ae8.json") %>%
  as_tibble()
view(nyc_trees)

# Get variable (column names)
colnames(nyc_trees)
```

*Aside on limits: Note that the full census dataset contains nearly 700,000 individual trees. However, we only downloaded a tiny sample of that, since the API defaults to a limit of 1,000 rows. You can override this default by adding ?$limit=LIMIT to the API endpoint. For example, to read in only the first five rows, you could use:*

```{r}
nyc_trees_5 <- fromJSON("https://data.cityofnewyork.us/resource/nwxe-4ae8.json?$limit=5")
```

`jsonlite::fromJSON()` automatically coerces everything into a character, so we’ll also need to convert some columns to numeric before we plot.

```{r}
nyc_trees %>% 
  select(longitude, latitude, stump_diam, spc_common, spc_latin, tree_id) %>% 
  mutate_at(vars(longitude:stump_diam), as.numeric) %>% 
  ggplot(aes(x=longitude, y=latitude, size=stump_diam)) + 
  geom_point(alpha=0.5, col = "blue") +
  scale_size_continuous(name = "Stump diameter") +
  labs(
    x = "Longitude", y = "Latitude",
    title = "Sample of New York City trees",
    caption = "Source: NYC Open Data"
    )+
  theme(legend.position = "bottom")
```

We’re going to go through the httr package. Why? Well, basically because httr comes with a variety of features that allow us to interact more flexibly and securely with web APIs. Let’s start by defining some convenience variables such as the endpoint path and the parameters (which we’ll store in a list).

```{r}
endpoint = "series/observations"

## Open your .Renviron file. Here we can add API keys that persist across R sessions.
#usethis::edit_r_environ() 
readRenviron("~/.Renviron") # Read the environment and your API

params = list(
  api_key= Sys.getenv("FRED_API_KEY"), ## Get API directly and safely from the stored environment variable
  file_type="json", 
  series_id="GNPCA"
  )
```

Next, we’ll use the `httr::GET()` function to request (i.e. download) the data. I’ll assign this to an object called fred.

```{r}
fred <- httr::GET(
  url = "https://api.stlouisfed.org/", ## Base URL
  path = paste0("fred/", endpoint), ## The API endpoint
  query = params ## Our parameter list
  )

fred
```

To extract the content (i.e. data) from of this response, I’ll use the `httr::content()` function. Moreover, since we know that this content is a JSON array, we can convert it to an R object using `jsonlite::fromJSON()` as we did above. With that being said, we don’t yet know what format the data will taken in R. (SPOILER: Okay, it will be a list.) I could use the base `str()` function to delve into the structure of the object. However, I want to take this opportunity to introduce you to the listviewer package `(link) ::jsonedit()`, which allows for interactive inspection of list objects.

```{r}
fred %>% 
  httr::content("text") %>%
  jsonlite::fromJSON() %>%
  listviewer::jsonedit(mode = "view")
```

I’ll re-run most of the above code and then extract this element. I could do this in several ways, but will use the purrr::pluck() function here.

```{r}
fred <-
  fred %>% 
  httr::content("text") %>%
  jsonlite::fromJSON() %>%
  purrr::pluck("observations") %>% ## Extract the "$observations" list element
  # .$observations %>% ## I could also have used this
  # magrittr::extract("observations") %>% ## Or this
  as_tibble() ## Just for nice formatting
fred
```

Recall that `jsonlite::fromJSON()` automatically converts everything to characters, so I’ll quickly change some variables to dates (using `lubridate::ymd())` and numeric.

```{r}
# library(lubridate) ## Already loaded above

fred <-
  fred %>%
  mutate_at(vars(realtime_start:date), ymd) %>%
  mutate(value = as.numeric(value)) 
```

And lets plot:
```{r}
fred %>%
  ggplot(aes(date, value)) +
  geom_line() +
  scale_y_continuous(labels = scales::comma) +
  labs(
    x="Date", y="2012 USD (Billions)",
    title="US Real Gross National Product", caption="Source: FRED"
    )
```


## Hidden API
*Our final application will involve a more challenging case where the API endpoint is hidden from view.*

### Going the old way with `rvest`

```{r}
library(rvest)

hx <- read_html("https://www.world.rugby/rankings/mru")

hx%>%
  html_nodes("body > section > div.pageContent > div:nth-child(2) > div.column.large-8 > div > section > section > div.fullRankingsContainer.large-7.columns > div > div > table")%>%
  html_table(fill = TRUE)%>%
  bind_rows()%>%
  as_tibble()

```

> As we see theold way does not really work well. 

### Locating the hidden API endpoint

Fortunately, there’s a better way: Access the full database of rankings through the API. First we have to find the endpoint, though.

![](pics/inspect-rugby.gif)

- Start by inspecting the page. (Ctr+Shift+I in Chrome)

- Head to the Network tab at the top of the inspect element panel.

- Click on the XHR button.10

- Refresh the page (Ctrl+R). This will allow us to see all the web traffic coming to and from the page in our inspect panel.

- Our task now is to scroll these different traffic links and see which one contains the information that we’re after.

- The top traffic link item references a URL called https://cmsapi.pulselive.com/rugby/rankings/mru?language=en&client=pulse. Hmmm. “API” you say? “Rankings” you say? Sounds promising…

- Click on this item and open up the Preview tab. In this case, we can see what looks to be the first row of the rankings table (“New Zealand”, etc.)

- To make sure, you can grab that  https://cmsapi.pulselive.com/rugby/rankings/mru?language=en&client=pulse, and paste it into our browser (using the JSONView plugin) from before.

```{r}
endpoint <- "https://cmsapi.pulselive.com/rugby/rankings/mru?language=en&client=pulse"
rugby <- fromJSON(endpoint)
str(rugby)

#str(rugby$entries) ## Base option
listviewer::jsonedit(rugby, mode = "view")
```

```{r}
head(rugby$entries$team)
```

It looks like we can just bind the columns of the `rugby$entries$team` data frame directly to the other elements of the parent $team "data frame" (actually: “list”). Let’s do that using `dplyr::bind_cols()` and then clean things up a bit. I’m going to call the resulting data frame rankings.

```{r}
# library(janitor) ## Already loaded above

rankings <- bind_cols(
  rugby$entries$team,
  rugby$entries %>% select(matches:previousPos)
  ) %>%
  clean_names() %>%
  select(-c(id, alt_id, annotations)) %>% ## These columns aren't adding much of interest
  select(pos, pts, everything()) %>% ## Reorder remaining columns
  as_tibble() ## "Enhanced" tidyverse version of a data frame
rankings
colnames(rankings)
```


## BONUS: Get and plot the rankings history

```{r}
## We'll look at rankings around Jan 1st each year. I'll use 2004 as an
## arbitrary start year and then proceed until the present year.
start_date <- ymd("2004-01-01")
end_date <- floor_date(today(), unit="years")
dates <- seq(start_date, end_date, by="years")
## Get the nearest Monday to Jan 1st to coincide with rankings release dates.
dates <- floor_date(dates, "week", week_start = getOption("lubridate.week.start", 1))
dates
```


Next, I’ll write out a function that I’ll call rugby_scrape. This function will take a single argument: a date that it will use to construct a new API endpoint during each iteration. Beyond that, it will pretty do much exactly the same things that we did in our previous, manual data scrape. The only other difference is that it will wait three seconds after running (i.e. Sys.sleep(3)). I’m adding this final line to avoid hammering the server with instantaneous requests when we put everything into a loop.

```{r}
## First remove our existing variables. This is not really necessary, since R is smart enough
## to distinguish named objects in functions from named objects in our global environment.
## But I want to emphasise that we're creating new data here and avoid any confusion.
rm(rugby, rankings, endpoint)

## Now, create the function. I'll call it "rugby_scrape".
rugby_scrape <- 
  function(x) {
    endpoint <- paste0("https://cmsapi.pulselive.com/rugby/rankings/mru?date=", x, "&client=pulse")
    rugby <- fromJSON(endpoint)
    rankings <-
      bind_cols(
        rugby$entries$team,
        rugby$entries %>% select(matches:previousPos)
        ) %>%
      clean_names() %>%
      mutate(date = x) %>% ## New column to keep track of the date 
      select(-c(id, alt_id, annotations)) %>% ## These columns aren't adding much of interest
      select(date, pos, pts, everything()) %>% ## Reorder remaining columns
      as_tibble() ## "Enhanced" tidyverse version of a data frame
    Sys.sleep(3) ## Be nice!
    return(rankings)
  }
```

Finally, we can now iterate (i.e. loop) over our dates vector, by plugging the values sequentially into our rugby_scrape function. There are a variety of ways to iterate in R, but I’m going to use an lapply() call below.12 We’ll then bind everything into a single data frame using dplyr::bind_rows() and name the resulting object rankings_history.

```{r}
rankings_history <-
  lapply(dates, rugby_scrape) %>% ## Run the iteration
  bind_rows() ## Bind the resulting list of data frames into a single data frame
rankings_history
```

```{r}
teams <- c("NZL", "RSA", "ENG", "JPN")
team_cols <- c("NZL"="black", "RSA"="#4DAF4A", "ENG"="#377EB8", "JPN" = "red")

rankings_history %>%
  ggplot(aes(x=date, y=pts, group=abbreviation)) +
  geom_line(col = "grey") + 
  geom_line(
    data = rankings_history %>% filter(abbreviation %in% teams), 
    aes(col=fct_reorder2(abbreviation, date, pts)),
    lwd = 1
    ) +
  scale_color_manual(values = team_cols) +
  labs(
    x = "Date", y = "Points", 
    title = "International rugby rankings", caption = "Source: World Rugby"
    ) +
  theme(legend.title = element_blank())
```






